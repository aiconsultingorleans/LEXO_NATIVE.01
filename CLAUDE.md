# ü§ñ CLAUDE.md - Guide Technique LEXO_NATIVE.01

## üìå Contexte Projet

**LEXO_NATIVE.01** : Application SaaS locale de gestion administrative intelligente qui automatise le traitement documentaire pour professions lib√©rales.

**Environnement Dev :** MacBook Pro M4 Pro Max (128GB RAM) | **Cible :** Mac Mini M4 (32GB RAM)  
**Architecture :** 100% native macOS avec pipeline IA optimis√© Apple Silicon  
**√âtat :** MVP op√©rationnel - Pipeline documentaire fonctionnel (89.7% pr√©cision)

---

## üèóÔ∏è Architecture Native macOS

### Structure Projet
```
~/Documents/LEXO_v1/IA_Administratif/
‚îú‚îÄ‚îÄ backend/                    # FastAPI (port 8000)
‚îÇ   ‚îú‚îÄ‚îÄ api/                   # Endpoints API
‚îÇ   ‚îú‚îÄ‚îÄ ocr/                   # Pipeline OCR hybride
‚îÇ   ‚îú‚îÄ‚îÄ rag/                   # Mistral MLX + ChromaDB
‚îÇ   ‚îú‚îÄ‚îÄ core/                  # Config, DB, Auth
‚îÇ   ‚îú‚îÄ‚îÄ test_models_selection.py  # Benchmark DONUT
‚îÇ   ‚îî‚îÄ‚îÄ main.py               # Point d'entr√©e
‚îú‚îÄ‚îÄ frontend/                   # Next.js (port 3000)
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ app/              # Pages Next.js
‚îÇ       ‚îî‚îÄ‚îÄ components/       # Composants React
‚îú‚îÄ‚îÄ ai_services/               # Mistral MLX (port 8004)
‚îÇ   ‚îú‚îÄ‚îÄ document_analyzer.py  # Service principal
‚îÇ   ‚îî‚îÄ‚îÄ venv/                 # Environnement MLX
‚îú‚îÄ‚îÄ ml_models/                 # Mod√®les locaux
‚îú‚îÄ‚îÄ data/                     # ChromaDB + Cache
‚îî‚îÄ‚îÄ OCR/                      # Dossier surveill√©
```

### Services Actifs
| Service | Port | Type | Fonction |
|---------|------|------|----------|
| **Backend FastAPI** | 8000 | uvicorn | API, OCR, Classification |
| **Frontend Next.js** | 3000 | npm dev | Interface utilisateur |
| **Mistral MLX** | 8004 | Python | Analyse IA Apple Silicon |
| **Donut + CamemBERT** | 8005 | Python | Pipeline alternatif (en dev) |
| **PostgreSQL** | 5432 | Homebrew | Base donn√©es |
| **Redis** | 6379 | Homebrew | Cache + queues |
| **ChromaDB** | - | Standalone | Base vectorielle |

---

## üîß Guidelines de D√©veloppement LEXO

### üéØ Philosophie g√©n√©rale

**KISS (Keep It Simple, Stupid)** - La simplicit√© est la sophistication ultime. Pr√©f√©rer toujours la solution la plus simple qui fonctionne.

### üìã R√®gles fondamentales

#### 1. **Pas de sur-ing√©nierie**
- ‚ùå PAS de patterns complexes si non n√©cessaires (pas de factories abstraites pour cr√©er un simple objet)
- ‚ùå PAS d'architecture sur-dimensionn√©e (pas de microservices pour une app desktop)
- ‚úÖ Code direct et lisible plut√¥t que "clever"
- ‚úÖ YAGNI (You Aren't Gonna Need It) - Ne pas coder pour des besoins futurs hypoth√©tiques

#### 2. **Architecture minimaliste**
```
app/
‚îú‚îÄ‚îÄ main/           # Code Swift/Objective-C pour macOS
‚îú‚îÄ‚îÄ backend/        # API Python simple (FastAPI/Flask)
‚îú‚îÄ‚îÄ frontend/       # Next.js si vraiment n√©cessaire
‚îî‚îÄ‚îÄ shared/         # Types et utilitaires partag√©s
```

#### 3. **D√©pendances minimales**
- Utiliser les biblioth√®ques natives macOS quand possible
- Pour Python : stdlib > petite lib bien maintenue > grosse framework
- Pour Next.js : √©viter les meta-frameworks et plugins non essentiels
- Chaque d√©pendance doit √™tre justifi√©e

### üêç Python - R√®gles sp√©cifiques

#### Structure simple
```python
# ‚ùå √âVITER
class AbstractLLMProviderFactory:
    def create_provider(self, config: Dict) -> AbstractLLMProvider:
        # 50 lignes de code...

# ‚úÖ PR√âF√âRER
def create_llm_client(api_key: str, model: str = "gpt-4"):
    return OpenAI(api_key=api_key, model=model)
```

#### Gestion des LLM
- Utiliser directement les SDK officiels (OpenAI, Anthropic, etc.)
- Pas de wrapper abstrait sauf si 2+ providers
- Gestion d'erreur simple avec try/except
- Logs clairs et utiles

#### API Backend
```python
# FastAPI minimal
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()

class Query(BaseModel):
    text: str
    model: str = "gpt-4"

@app.post("/process")
async def process_document(query: Query):
    try:
        # Code direct, pas de couches d'abstraction
        result = llm_client.complete(query.text)
        return {"result": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### üñ•Ô∏è macOS Native - R√®gles sp√©cifiques

#### Swift/SwiftUI
- Utiliser SwiftUI sauf besoin sp√©cifique AppKit
- Pas de MVVM complexe si MV suffit
- Utiliser les APIs natives (NSDocument, etc.)
- √âviter les biblioth√®ques tierces pour l'UI

#### Communication avec le backend
```swift
// ‚ùå √âVITER - Sur-abstraction
protocol NetworkManagerProtocol { }
class NetworkManager: NetworkManagerProtocol { }
class APIClient { }
class LLMService { }

// ‚úÖ PR√âF√âRER - Direct et simple
class APIClient {
    func processDocument(_ text: String) async throws -> String {
        // URLSession direct
    }
}
```

#### Int√©gration Python
- Utiliser un subprocess Python ou
- API REST locale (http://localhost:8000)
- Pas de bridge complexe Python-Swift

### ‚öõÔ∏è Next.js (si n√©cessaire) - R√®gles sp√©cifiques

#### Quand l'utiliser
- ‚ùì Se demander d'abord : "Ai-je vraiment besoin de Next.js ?"
- Une WebView SwiftUI simple peut suffire
- Si oui : App Router simple, pas de magie

#### Structure minimale
```
frontend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx        # Page unique si possible
‚îÇ   ‚îî‚îÄ‚îÄ api/            # Seulement si n√©cessaire
‚îú‚îÄ‚îÄ components/         # Composants simples
‚îî‚îÄ‚îÄ lib/               # Utilitaires
```

#### Composants
```tsx
// ‚ùå √âVITER
const DocumentProcessor: FC<DocumentProcessorProps> = ({ 
  config, 
  callbacks, 
  providers 
}) => {
  // 200 lignes de hooks custom...
}

// ‚úÖ PR√âF√âRER
function DocumentView({ document, onProcess }) {
  const [result, setResult] = useState(null);
  
  async function handleProcess() {
    const res = await fetch('/api/process', {
      method: 'POST',
      body: JSON.stringify({ text: document })
    });
    setResult(await res.json());
  }
  
  return (
    <div>
      {/* UI simple et directe */}
    </div>
  );
}
```

### üîß Patterns √† utiliser

#### 1. **Configuration simple**
```python
# config.py
import os
from dataclasses import dataclass

@dataclass
class Config:
    api_key: str = os.getenv("OPENAI_API_KEY", "")
    model: str = "gpt-4"
    max_tokens: int = 2000

config = Config()
```

#### 2. **Gestion d'erreur pragmatique**
```python
def process_with_llm(text: str) -> str:
    try:
        return llm_client.complete(text)
    except RateLimitError:
        time.sleep(1)
        return process_with_llm(text)  # Retry simple
    except Exception as e:
        logger.error(f"LLM error: {e}")
        return "Erreur de traitement"
```

#### 3. **Tests simples**
```python
# test_simple.py
def test_document_processing():
    result = process_document("test")
    assert result is not None
    assert len(result) > 0
```

### üö´ Anti-patterns √† √©viter

1. **Dependency Injection complexe** pour une app desktop
2. **Event Sourcing / CQRS** - Overkill
3. **GraphQL** quand REST suffit
4. **Kubernetes/Docker** pour dev local
5. **Redux/MobX** quand useState suffit
6. **TypeScript strict partout** - Seulement o√π c'est utile
7. **Tests unitaires de tout** - Tester ce qui a de la valeur

### üìù Checklist avant d'ajouter de la complexit√©

- [ ] Est-ce que la solution simple a √©t√© essay√©e ?
- [ ] Est-ce que cette abstraction sera utilis√©e plus d'une fois ?
- [ ] Est-ce que √ßa rend le code plus lisible ?
- [ ] Est-ce que √ßa facilite vraiment la maintenance ?
- [ ] Est-ce que c'est n√©cessaire MAINTENANT ?

### üé™ Exemples de simplification

#### Gestion d'√©tat
```python
# ‚ùå √âVITER
class StateManager:
    def __init__(self):
        self._observers = []
        self._state = {}
    
    def subscribe(self, observer):
        # ...

# ‚úÖ PR√âF√âRER
app_state = {
    "current_document": None,
    "processing": False
}
```

#### Appels LLM
```python
# ‚ùå √âVITER
class LLMChain:
    def __init__(self, prompt_template, parser, memory):
        # ...

# ‚úÖ PR√âF√âRER
def ask_llm(question: str, context: str = "") -> str:
    prompt = f"{context}\n\nQuestion: {question}"
    return openai_client.complete(prompt)
```

### üèÅ R√©sum√©

1. **√âcrire du code boring** - Facile √† lire, facile √† d√©bugger
2. **Commencer simple** - Complexifier seulement si n√©cessaire
3. **Pr√©f√©rer la composition** √† l'h√©ritage
4. **Une fonction = une responsabilit√©**
5. **Si c'est difficile √† expliquer, c'est trop complexe**

### üí° Mantra final

> "Le meilleur code est celui qu'on n'a pas √©crit. Le deuxi√®me meilleur est celui qu'un junior peut comprendre en 5 minutes."

**Note pour Claude Code** : Ces guidelines sont prioritaires. En cas de doute, choisir la simplicit√©. Ne pas sugg√©rer de patterns complexes sauf demande explicite. Proposer toujours la solution la plus directe en premier.

---

## üõ°Ô∏è Approche Non-Destructive Garantie

### Principe Fondamental
- **AUCUNE suppression de code** sans validation explicite utilisateur
- **Conservation totale** des pipelines fonctionnels (Mistral MLX reste principal)
- **D√©veloppement en parall√®le** sans impact sur l'existant
- **Rollback instantan√©** toujours possible

### Workflow S√©curis√©
- **Branches isol√©es** par feature (`feat/donut_alternative_pipeline`)
- **Services parall√®les** sur ports distincts (8004 Mistral, 8005 DONUT)
- **Tests non-destructifs** sans remplacement du pipeline principal
- **Validation utilisateur** obligatoire avant toute suppression
- **Conservation API** existante intacte

### R√®gles de D√©veloppement
1. **Ajout uniquement** : Nouvelles fonctionnalit√©s en plus de l'existant
2. **Coexistence** : Anciens et nouveaux syst√®mes fonctionnent ensemble
3. **Toggle utilisateur** : Choix entre pipelines depuis l'interface
4. **Fallback automatique** : Retour vers Mistral en cas de probl√®me DONUT
5. **Tests isol√©s** : Validation sans affecter le production
6. **Documentation** : Chaque ajout document√© pour tra√ßabilit√©

---

## ‚ö° Pipeline Documentaire

### Flux Principal (Non-Destructif)
```
üìÑ Upload ‚Üí üîç OCR Hybride ‚Üí ü§ñ [Mistral MLX (principal) | DONUT+CamemBERT (alternatif)] ‚Üí üè∑Ô∏è Classification ‚Üí üìÅ Classement Auto
```

**Toggle utilisateur** : Choix du pipeline depuis le dashboard, Mistral MLX reste par d√©faut

### API Endpoints Cl√©s
```bash
# Pipeline unifi√© (RECOMMAND√â)
POST /api/v1/documents/upload-and-process    # Upload + OCR + IA + Classification

# Composants individuels  
POST /api/v1/ocr/process                    # OCR hybride
POST /api/v1/intelligence/analyze          # Analyse Mistral
POST /api/v1/classification/classify       # Classification

# RAG + Recherche
POST /api/v1/rag/search                     # Recherche s√©mantique
POST /api/v1/rag/chat                       # Chat avec contexte
```

---

## üöÄ D√©marrage Native

### Commandes Essentielles
```bash
# D√©marrage complet (30 secondes)
cd ~/Documents/LEXO_v1/IA_Administratif
./start_native.sh

# Arr√™t propre
./stop_native.sh

# Services individuels
./start_backend_native.sh     # Backend seul
./start_frontend_native.sh    # Frontend seul

# Diagnostic
./diagnostic_native.sh        # √âtat syst√®me complet
```

### Performance D√©marrage
- **Services syst√®me** : Instantan√© (Homebrew)
- **API Backend** : ~10s (vs 30s Docker)
- **Interface web** : ~5s 
- **Mistral MLX** : ~30s (vs 60s Docker)
- **Pipeline complet** : **30-40s total**

---

## üéØ √âtat Avancement

### ‚úÖ COMPL√âT√âES
- **√âtapes 1-2** : Backend + Frontend (100%)
- **√âtape 3** : Pipeline OCR hybride (82%)
- **√âtape 4** : Classification 9 cat√©gories (100%)
- **√âtape 5** : RAG + Mistral MLX (95%)
- **√âtape 7** : Dashboard Analytics (95%)

### üöß EN COURS (Approche Non-Destructive)
- **Pipeline DONUT** : Alternative compl√©mentaire Mistral (√âtape 1/7 compl√©t√©e)
- **√âtape 6** : Int√©grations externes (20%)
- **√âtape 9** : S√©curit√© + Performance (40%)

**Note** : Tous d√©veloppements conservent int√©gralement le pipeline Mistral MLX existant

---

## üîß Stack Technologique

### Backend Native
```python
# FastAPI 0.115+ avec uvicorn Apple Silicon
# SQLAlchemy 2.0 + PostgreSQL 15 Homebrew
# Redis 7 (cache + queues)
# Alembic migrations

# OCR Pipeline
# - TrOCR (HuggingFace) ARM64 optimis√©
# - Tesseract 5 Homebrew (fallback)
# - LayoutLMv3 (structure)

# IA Apple Silicon
# - Mistral 7B MLX (optimisation M4)
# - ChromaDB standalone
# - Sentence-Transformers ARM64

# Pipeline Alternatif DONUT (en d√©veloppement)
# - Donut OCR-free (naver-clova-ix/donut-base-finetuned-cord-v2)
# - CamemBERT fran√ßais (almanach/camembert-base)
# - NER fran√ßais (Jean-Baptiste/camembert-ner)
```

### Frontend Moderne
```typescript
// Next.js 15 + React 19
// TypeScript strict + Tailwind CSS 4
// Zustand (state) + React Hook Form
// Composants optimis√©s hot reload
```

---

## üìÅ Navigation Rapide - Fichiers Cl√©s

### Configuration & API
```bash
backend/core/config.py         # Configuration principale
backend/main.py               # Point d'entr√©e FastAPI
backend/api/documents.py      # Upload unifi√©
backend/api/health.py         # Health checks
```

### Pipeline Documentaire
```bash
backend/ocr/hybrid_ocr.py     # OCR principal
backend/rag/mistral_wrapper.py # Interface Mistral MLX
backend/services/document_classifier.py # Classification
backend/test_models_selection.py # Benchmark DONUT
backend/api/donut_endpoints.py    # API DONUT (planifi√©)
ai_services/donut_camembert_analyzer.py # Service DONUT (planifi√©)
```

### Frontend Interface
```bash
frontend/src/app/dashboard/page.tsx       # Dashboard principal
frontend/src/components/documents/        # Upload + liste
frontend/src/components/dashboard/        # Analytics KPI
frontend/src/services/api.ts              # Client API
```

### Services IA
```bash
ai_services/document_analyzer.py         # Service Mistral MLX
ml_models/mistral_7b_mlx/                # Mod√®les locaux
data/chromadb/                           # Base vectorielle
```

---

## üìä Fonctionnalit√©s Op√©rationnelles

### 1. Upload + Traitement Intelligent (Non-Destructif)
- **Drag & Drop** : PDF, images (PNG, JPG, TIFF)
- **Pipeline unifi√©** : Upload ‚Üí OCR ‚Üí [Mistral MLX (d√©faut) | DONUT (alternatif)] ‚Üí Classification
- **Toggle utilisateur** : Choix pipeline dans dashboard, Mistral conserv√© par d√©faut
- **Performance** : <10 secondes par document
- **Fallback automatique** : Retour vers Mistral en cas d'erreur DONUT

### 2. Classification Automatique
```typescript
Categories = {
  factures, rib, contrats, attestations, 
  courriers, rapports, cartes_transport,
  documents_personnels, non_classes
}
// Score moyen: 89.7% de confiance
```

### 3. Dashboard Analytics
- **KPIs temps r√©el** : Documents trait√©s, pr√©cision OCR
- **Graphiques** : Bar, Line, Pie charts (Recharts)
- **Timeline** : Activit√© r√©cente
- **Filtres** : Date, cat√©gorie, statut

### 4. RAG + Chat Intelligent  
- **Recherche s√©mantique** : ChromaDB + embeddings
- **Chat contexte** : Mistral MLX avec sources
- **Performance** : <100ms retrieval, <2s g√©n√©ration

---

## üß™ Tests Valid√©s

### Scripts Principaux
```bash
cd backend && source venv/bin/activate

# Tests pipeline
python test_ocr_etape3.py        # OCR (82% succ√®s)
python test_rag_etape5.py        # RAG (95% succ√®s)  
python test_models_selection.py # Benchmark DONUT

# Tests sp√©cifiques
pytest tests/                   # Tests unitaires
```

### Validation Documents R√©els
- **Factures EDF** : Classification + extraction entit√©s ‚úÖ
- **RIB bancaires** : D√©tection + classement automatique ‚úÖ
- **Attestations** : CPAM, CAF d√©tect√©es ‚úÖ

---

## üîÑ Hot Reload D√©veloppement

### Workflow Optimis√©
```bash
# D√©marrage une fois
./start_native.sh

# D√©veloppement continu - pas de red√©marrage
# - Composants React : HMR instantan√© (<500ms)
# - API Python : uvicorn --reload automatique (<1s)
# - Styles CSS : Hot reload instantan√©
```

### Cas Red√©marrage N√©cessaire
- **Frontend** : Modification `package.json` ‚Üí `npm install`
- **Backend** : Modification `requirements.txt` ‚Üí `pip install`
- **Complet** : `./stop_native.sh && ./start_native.sh`

---

## üõ°Ô∏è Stabilit√© & Performance

### Auto-correction Native
- **Services Homebrew** : D√©marrage automatique PostgreSQL/Redis
- **Environnements virtuels** : Cr√©ation automatique si manquants
- **D√©pendances** : V√©rification et installation auto

### Protection Donn√©es
- **Sauvegarde auto** : Lors arr√™t dans `logs/backups/YYYYMMDD/`
- **V√©rification batch** : Pas d'arr√™t si traitements en cours
- **Logs centralis√©s** : `logs/backend_native.log`, `logs/mistral_native.log`

---

**üìà √âtat :** MVP natif op√©rationnel - Pipeline Mistral MLX fonctionnel (89.7% pr√©cision)  
**üéØ Focus :** Pipeline DONUT alternatif en d√©veloppement (√âtape 1/7 compl√©t√©e - Mod√®les valid√©s)  
**üõ°Ô∏è Approche :** 100% Non-Destructive - Conservation totale de l'existant

*Derni√®re mise √† jour : 27 juillet 2025 - Guidelines + Approche Non-Destructive int√©gr√©es*