#!/usr/bin/env python3
"""
üöÄ LEXO v1 - Migration des Mod√®les ML vers Cache Local
====================================================

Ce script migre tous les mod√®les ML depuis le cache HuggingFace syst√®me
vers le cache local LEXO pour √©liminer les t√©l√©chargements au d√©marrage.

Usage:
    python migrate_models_to_local_cache.py [--dry-run] [--force]
    
Arguments:
    --dry-run    : Simulation sans copie r√©elle
    --force      : √âcraser les mod√®les existants
"""

import os
import sys
import shutil
import hashlib
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import time

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ModelCacheMigrator:
    """Gestionnaire de migration du cache des mod√®les ML"""
    
    # Mod√®les requis par LEXO v1
    REQUIRED_MODELS = {
        'transformers': [
            {
                'hf_name': 'microsoft--trocr-base-printed',
                'local_name': 'trocr-base-printed',
                'description': 'OCR TrOCR pour texte imprim√©',
                'size_expected': '1.2GB'
            },
            {
                'hf_name': 'sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2',
                'local_name': 'paraphrase-multilingual-MiniLM-L12-v2', 
                'description': 'Embeddings multilingues pour RAG',
                'size_expected': '458MB'
            }
        ],
        'spacy': [
            {
                'name': 'fr_core_news_sm',
                'description': 'Mod√®le spaCy fran√ßais pour NER',
                'size_expected': '15MB'
            }
        ]
    }
    
    def __init__(self, dry_run: bool = False, force: bool = False):
        """
        Initialise le migrateur
        
        Args:
            dry_run: Mode simulation sans actions r√©elles
            force: √âcraser les mod√®les existants
        """
        self.dry_run = dry_run
        self.force = force
        
        # Chemins de cache
        self.project_root = Path(__file__).parent.parent
        self.hf_cache_dir = Path.home() / '.cache' / 'huggingface' / 'hub'
        self.local_cache_dir = self.project_root / 'ml_models'
        
        # Sous-r√©pertoires
        self.transformers_cache = self.local_cache_dir / 'transformers'
        self.spacy_cache = self.local_cache_dir / 'spacy'
        
        # Stats de migration
        self.stats = {
            'models_found': 0,
            'models_migrated': 0,
            'total_size_copied': 0,
            'errors': []
        }
    
    def validate_environment(self) -> bool:
        """Valide l'environnement de migration"""
        logger.info("üîç Validation de l'environnement...")
        
        # V√©rifier cache HuggingFace syst√®me
        if not self.hf_cache_dir.exists():
            logger.error(f"‚ùå Cache HuggingFace non trouv√©: {self.hf_cache_dir}")
            return False
        
        logger.info(f"‚úÖ Cache HuggingFace syst√®me: {self.hf_cache_dir}")
        
        # Cr√©er r√©pertoires locaux si n√©cessaire
        if not self.dry_run:
            self.local_cache_dir.mkdir(exist_ok=True)
            self.transformers_cache.mkdir(exist_ok=True)
            self.spacy_cache.mkdir(exist_ok=True)
            logger.info(f"‚úÖ Cache local pr√©par√©: {self.local_cache_dir}")
        
        return True
    
    def find_model_in_cache(self, hf_model_name: str) -> Optional[Path]:
        """Trouve un mod√®le dans le cache HuggingFace syst√®me"""
        model_path = self.hf_cache_dir / f"models--{hf_model_name}"
        
        if not model_path.exists():
            logger.warning(f"  ‚ùå Mod√®le manquant: {hf_model_name}")
            return None
        
        # Trouver le dernier snapshot
        snapshots_dir = model_path / "snapshots"
        if not snapshots_dir.exists():
            logger.warning(f"  ‚ùå Pas de snapshots pour: {hf_model_name}")
            return None
        
        # Prendre le premier (et g√©n√©ralement seul) snapshot
        snapshots = list(snapshots_dir.iterdir())
        if not snapshots:
            logger.warning(f"  ‚ùå Aucun snapshot disponible: {hf_model_name}")
            return None
        
        # Utiliser le snapshot le plus r√©cent
        latest_snapshot = max(snapshots, key=lambda p: p.stat().st_mtime)
        logger.info(f"  ‚úÖ Mod√®le trouv√©: {hf_model_name} (snapshot: {latest_snapshot.name[:8]}...)")
        
        return latest_snapshot
    
    def get_directory_size(self, path: Path) -> int:
        """Calcule la taille totale d'un r√©pertoire"""
        total_size = 0
        for file_path in path.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return total_size
    
    def format_size(self, size_bytes: int) -> str:
        """Formate une taille en bytes en format lisible"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f}{unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f}TB"
    
    def copy_model_with_progress(self, source: Path, destination: Path, model_name: str) -> bool:
        """Copie un mod√®le avec barre de progression"""
        try:
            if destination.exists() and not self.force:
                logger.info(f"  ‚ö†Ô∏è  Mod√®le d√©j√† pr√©sent: {model_name} (utilisez --force pour √©craser)")
                return True
            
            if self.dry_run:
                size = self.get_directory_size(source)
                logger.info(f"  üîÑ [DRY-RUN] Copierait {model_name}: {self.format_size(size)}")
                return True
            
            # Supprimer destination si existe et force activ√©
            if destination.exists() and self.force:
                logger.info(f"  üóëÔ∏è  Suppression ancienne version: {model_name}")
                shutil.rmtree(destination)
            
            # Cr√©er r√©pertoire parent
            destination.parent.mkdir(parents=True, exist_ok=True)
            
            # Copie avec timing
            start_time = time.time()
            logger.info(f"  üîÑ Copie en cours: {model_name}...")
            
            shutil.copytree(source, destination)
            
            copy_time = time.time() - start_time
            size = self.get_directory_size(destination)
            
            logger.info(f"  ‚úÖ Mod√®le copi√©: {model_name}")
            logger.info(f"     üìä Taille: {self.format_size(size)}")
            logger.info(f"     ‚è±Ô∏è  Temps: {copy_time:.2f}s")
            
            self.stats['total_size_copied'] += size
            return True
            
        except Exception as e:
            error_msg = f"Erreur copie {model_name}: {str(e)}"
            logger.error(f"  ‚ùå {error_msg}")
            self.stats['errors'].append(error_msg)
            return False
    
    def migrate_transformers_models(self) -> bool:
        """Migre les mod√®les HuggingFace/Transformers"""
        logger.info("ü§ñ Migration des mod√®les Transformers...")
        
        success_count = 0
        
        for model_info in self.REQUIRED_MODELS['transformers']:
            hf_name = model_info['hf_name']
            local_name = model_info['local_name']
            description = model_info['description']
            
            logger.info(f"üì¶ Traitement: {description}")
            
            # Trouver dans cache syst√®me
            source_path = self.find_model_in_cache(hf_name)
            if not source_path:
                continue
            
            # D√©finir destination
            destination_path = self.transformers_cache / local_name
            
            # Copier
            if self.copy_model_with_progress(source_path, destination_path, local_name):
                success_count += 1
                self.stats['models_migrated'] += 1
        
        logger.info(f"‚úÖ Mod√®les Transformers migr√©s: {success_count}/{len(self.REQUIRED_MODELS['transformers'])}")
        return success_count == len(self.REQUIRED_MODELS['transformers'])
    
    def migrate_spacy_models(self) -> bool:
        """Migre les mod√®les spaCy"""
        logger.info("üî§ Migration des mod√®les spaCy...")
        
        try:
            import spacy
            
            # Trouver l'installation spaCy
            spacy_data_dir = Path(spacy.util.get_data_path())
            
            success_count = 0
            
            for model_info in self.REQUIRED_MODELS['spacy']:
                model_name = model_info['name']
                description = model_info['description']
                
                logger.info(f"üì¶ Traitement: {description}")
                
                # Trouver mod√®le spaCy
                source_path = spacy_data_dir / model_name
                
                if not source_path.exists():
                    logger.warning(f"  ‚ùå Mod√®le spaCy non trouv√©: {model_name}")
                    logger.info(f"     üí° Installer avec: python -m spacy download {model_name}")
                    continue
                
                # Destination
                destination_path = self.spacy_cache / model_name
                
                # Copier
                if self.copy_model_with_progress(source_path, destination_path, model_name):
                    success_count += 1
                    self.stats['models_migrated'] += 1
            
            logger.info(f"‚úÖ Mod√®les spaCy migr√©s: {success_count}/{len(self.REQUIRED_MODELS['spacy'])}")
            return success_count > 0
            
        except ImportError:
            logger.warning("‚ö†Ô∏è  spaCy non install√© - mod√®les spaCy ignor√©s")
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur migration spaCy: {e}")
            return False
    
    def verify_migrated_models(self) -> bool:
        """V√©rifie l'int√©grit√© des mod√®les migr√©s"""
        if self.dry_run:
            logger.info("üîç [DRY-RUN] V√©rification des mod√®les simul√©e")
            return True
        
        logger.info("üîç V√©rification des mod√®les migr√©s...")
        
        all_valid = True
        
        # V√©rifier mod√®les Transformers
        for model_info in self.REQUIRED_MODELS['transformers']:
            local_name = model_info['local_name']
            model_path = self.transformers_cache / local_name
            
            if model_path.exists():
                # V√©rifier fichiers essentiels (TrOCR utilise safetensors)
                essential_files = ['config.json']
                model_files = ['model.safetensors', 'pytorch_model.bin']  # L'un des deux doit √™tre pr√©sent
                
                # V√©rifier config obligatoire
                config_valid = (model_path / 'config.json').exists()
                if not config_valid:
                    logger.error(f"  ‚ùå config.json manquant: {local_name}")
                    all_valid = False
                    continue
                
                # V√©rifier qu'au moins un fichier de mod√®le existe
                model_file_valid = any((model_path / f).exists() for f in model_files)
                if not model_file_valid:
                    logger.error(f"  ‚ùå Fichier de mod√®le manquant: {local_name} (cherch√©: {model_files})")
                    all_valid = False
                    continue
                
                logger.info(f"  ‚úÖ Mod√®le valide: {local_name}")
            else:
                logger.error(f"  ‚ùå Mod√®le manquant: {local_name}")
                all_valid = False
        
        # V√©rifier mod√®les spaCy
        for model_info in self.REQUIRED_MODELS['spacy']:
            model_name = model_info['name']
            model_path = self.spacy_cache / model_name
            
            if model_path.exists():
                logger.info(f"  ‚úÖ Mod√®le spaCy: {model_name}")
            else:
                logger.warning(f"  ‚ö†Ô∏è  Mod√®le spaCy manquant: {model_name}")
        
        return all_valid
    
    def generate_cache_config(self) -> None:
        """G√©n√®re la configuration de cache pour Docker"""
        if self.dry_run:
            logger.info("üìù [DRY-RUN] Configuration de cache simul√©e")
            return
        
        logger.info("üìù G√©n√©ration de la configuration de cache...")
        
        config_content = f"""# üöÄ LEXO v1 - Configuration Cache ML Local
# G√©n√©r√© automatiquement par migrate_models_to_local_cache.py

# Variables d'environnement pour cache local uniquement
TRANSFORMERS_CACHE=/app/ml_models/transformers
HF_MODELS_CACHE=/app/ml_models/transformers
HF_HUB_CACHE=/app/ml_models/transformers
SPACY_DATA_DIR=/app/ml_models/spacy

# Forcer utilisation cache local (pas de t√©l√©chargement)
HF_OFFLINE=1
TRANSFORMERS_OFFLINE=1

# D√©sactiver t√©l√©m√©trie
HF_HUB_DISABLE_TELEMETRY=1
"""
        
        config_path = self.project_root / 'config' / 'ml_cache.env'
        config_path.parent.mkdir(exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        logger.info(f"‚úÖ Configuration sauv√©e: {config_path}")
    
    def run_migration(self) -> bool:
        """Ex√©cute la migration compl√®te"""
        logger.info("üöÄ LEXO v1 - Migration des Mod√®les ML vers Cache Local")
        logger.info("=" * 60)
        
        if self.dry_run:
            logger.info("üîç MODE DRY-RUN: Simulation sans modification")
        
        # Validation environnement
        if not self.validate_environment():
            return False
        
        # Migration des mod√®les
        start_time = time.time()
        
        transformers_ok = self.migrate_transformers_models()
        spacy_ok = self.migrate_spacy_models()
        
        # V√©rification
        if transformers_ok or spacy_ok:
            verification_ok = self.verify_migrated_models()
        else:
            verification_ok = False
        
        # Configuration
        if verification_ok and not self.dry_run:
            self.generate_cache_config()
        
        # Rapport final
        total_time = time.time() - start_time
        
        logger.info("=" * 60)
        logger.info("üìä RAPPORT DE MIGRATION")
        logger.info(f"‚è±Ô∏è  Temps total: {total_time:.2f}s")
        logger.info(f"üì¶ Mod√®les migr√©s: {self.stats['models_migrated']}")
        logger.info(f"üíæ Taille totale: {self.format_size(self.stats['total_size_copied'])}")
        
        if self.stats['errors']:
            logger.info(f"‚ùå Erreurs: {len(self.stats['errors'])}")
            for error in self.stats['errors']:
                logger.info(f"   - {error}")
        
        success = verification_ok and len(self.stats['errors']) == 0
        
        if success:
            logger.info("‚úÖ Migration r√©ussie!")
            logger.info("üí° Prochaines √©tapes:")
            logger.info("   1. Modifier le code pour utiliser local_files_only=True")
            logger.info("   2. Mettre √† jour docker-compose.yml")
            logger.info("   3. Tester le d√©marrage rapide")
        else:
            logger.error("‚ùå Migration √©chou√©e - V√©rifier les erreurs ci-dessus")
        
        return success


def main():
    """Point d'entr√©e principal"""
    parser = argparse.ArgumentParser(
        description='Migration des mod√®les ML vers cache local LEXO v1'
    )
    parser.add_argument(
        '--dry-run', 
        action='store_true',
        help='Mode simulation sans copie r√©elle'
    )
    parser.add_argument(
        '--force',
        action='store_true', 
        help='√âcraser les mod√®les existants'
    )
    
    args = parser.parse_args()
    
    migrator = ModelCacheMigrator(dry_run=args.dry_run, force=args.force)
    
    try:
        success = migrator.run_migration()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  Migration interrompue par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()