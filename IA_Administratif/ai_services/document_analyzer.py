"""
Service d'analyse documentaire intelligent avec Mistral MLX
Analyse s√©mantique et classification avanc√©e de documents
"""

import asyncio
import logging
import time
import json
import re
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from pathlib import Path
from enum import Enum
from functools import wraps
import threading

# FastAPI pour l'API service
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# MLX pour Mistral
try:
    from mlx_lm import load, generate
except ImportError:
    logging.error("mlx_lm not installed. Run: pip install mlx-lm")
    raise

# Configuration logging d√©taill√©
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration timeouts
MISTRAL_TIMEOUT = 30  # secondes
MISTRAL_MAX_RETRIES = 2


class DocumentType(Enum):
    """Types de documents support√©s"""
    FACTURE = "facture"
    RIB = "rib"
    CONTRAT = "contrat"
    ATTESTATION = "attestation"
    COURRIER = "courrier"
    RAPPORT = "rapport"
    AUTRE = "autre"


class AnalysisType(Enum):
    """Types d'analyse disponibles"""
    CLASSIFICATION = "classification"
    SUMMARIZATION = "summarization"
    KEY_EXTRACTION = "key_extraction"
    SENTIMENT = "sentiment"
    COMPLIANCE = "compliance"


@dataclass
class DocumentAnalysisResult:
    """R√©sultat d'analyse documentaire"""
    document_type: DocumentType
    confidence: float
    summary: str
    key_information: Dict[str, Any]
    entities: List[Dict[str, Any]]
    sentiment: Optional[str] = None
    compliance_status: Optional[str] = None
    processing_time: float = 0.0
    raw_analysis: Optional[str] = None


class DocumentAnalysisRequest(BaseModel):
    """Requ√™te d'analyse de document"""
    text: str
    analysis_types: List[str] = ["classification", "key_extraction"]
    document_context: Optional[str] = None
    custom_prompt: Optional[str] = None


class ChatRequest(BaseModel):
    """Requ√™te de chat conversationnel"""
    message: str
    conversation_id: Optional[str] = "default"
    max_tokens: Optional[int] = 2000
    system_prompt: Optional[str] = None


class DocumentAnalyzer:
    """
    Analyseur de documents intelligent utilisant Mistral MLX
    """
    
    def __init__(self, model_path: str = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"):
        """
        Initialise l'analyseur de documents
        
        Args:
            model_path: Chemin vers le mod√®le Mistral MLX
        """
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.logger = logging.getLogger("document_analyzer")
        
        # Prompts sp√©cialis√©s
        self.prompts = {
            "classification": """Tu es un expert en analyse documentaire. Analyse le document suivant et d√©termine son type principal parmi : facture, RIB, contrat, attestation, courrier, rapport, autre.

Document √† analyser :
{text}

R√©ponds en JSON avec cette structure exacte :
{{"type": "type_du_document", "confidence": 0.95, "reasoning": "explication_courte"}}""",

            "key_extraction": """Tu es un expert en extraction d'informations. Extrais les informations cl√©s de ce document administratif fran√ßais.

Document :
{text}

Extrais et structure les informations importantes (dates, montants, noms, adresses, num√©ros de r√©f√©rence, etc.) en JSON strict avec cette structure exacte :
{{"dates": [], "montants": [], "personnes": [], "entreprises": [], "references": [], "autres": {{}}}}

R√©ponds UNIQUEMENT par le JSON, sans texte suppl√©mentaire.""",

            "summarization": """Tu es un expert en synth√®se documentaire. Cr√©√© un r√©sum√© concis et professionnel de ce document.

Document :
{text}

R√©sum√© en 2-3 phrases maximum, en fran√ßais professionnel, sans reprendre le prompt :""",

            "compliance": """Tu es un expert en conformit√© documentaire. √âvalue la conformit√© et la compl√©tude de ce document administratif fran√ßais.

Document :
{text}

Analyse en JSON :
{{"conformite": "conforme/non_conforme/partiel", "elements_manquants": [], "recommandations": []}}"""
        }
    
    async def initialize(self):
        """Initialise le mod√®le Mistral MLX"""
        try:
            self.logger.info(f"Chargement du mod√®le Mistral : {self.model_path}")
            self.model, self.tokenizer = load(self.model_path)
            self.logger.info("Document Analyzer initialis√© avec succ√®s")
        except Exception as e:
            self.logger.error(f"Erreur initialisation Document Analyzer : {e}")
            raise
    
    async def analyze_document(
        self, 
        text: str, 
        analysis_types: List[AnalysisType] = None,
        document_context: Optional[str] = None
    ) -> DocumentAnalysisResult:
        """
        Analyse compl√®te d'un document
        
        Args:
            text: Texte du document √† analyser
            analysis_types: Types d'analyse √† effectuer
            document_context: Contexte additionnel du document
            
        Returns:
            R√©sultat d'analyse structur√©
        """
        start_time = time.time()
        
        if analysis_types is None:
            analysis_types = [AnalysisType.CLASSIFICATION, AnalysisType.KEY_EXTRACTION]
        
        try:
            result = DocumentAnalysisResult(
                document_type=DocumentType.AUTRE,
                confidence=0.0,
                summary="",
                key_information={},
                entities=[],
                processing_time=0.0
            )
            
            # Classification du document
            if AnalysisType.CLASSIFICATION in analysis_types:
                classification = await self._classify_document(text)
                result.document_type = classification.get("type", DocumentType.AUTRE)
                result.confidence = classification.get("confidence", 0.0)
            
            # Extraction d'informations cl√©s
            if AnalysisType.KEY_EXTRACTION in analysis_types:
                key_info = await self._extract_key_information(text)
                result.key_information = key_info
            
            # R√©sum√©
            if AnalysisType.SUMMARIZATION in analysis_types:
                result.summary = await self._summarize_document(text)
            
            # Analyse de conformit√©
            if AnalysisType.COMPLIANCE in analysis_types:
                compliance = await self._analyze_compliance(text)
                result.compliance_status = compliance.get("conformite", "non_√©valu√©")
            
            result.processing_time = time.time() - start_time
            
            self.logger.info(f"Analyse termin√©e en {result.processing_time:.2f}s - Type: {result.document_type.value if hasattr(result.document_type, 'value') else result.document_type} (conf: {result.confidence:.2f})")
            return result
            
        except Exception as e:
            self.logger.error(f"Erreur lors de l'analyse documentaire : {e}")
            raise HTTPException(status_code=500, detail=f"Erreur d'analyse : {str(e)}")
    
    def _timeout_wrapper(self, func, *args, **kwargs):
        """Wrapper pour ajouter timeout aux g√©n√©rations Mistral"""
        result = {'value': None, 'error': None}
        
        def target():
            try:
                result['value'] = func(*args, **kwargs)
            except Exception as e:
                result['error'] = e
        
        thread = threading.Thread(target=target)
        thread.start()
        thread.join(timeout=MISTRAL_TIMEOUT)
        
        if thread.is_alive():
            self.logger.error(f"Timeout Mistral apr√®s {MISTRAL_TIMEOUT}s")
            # Note: Impossible de killer proprement le thread MLX
            raise TimeoutError(f"G√©n√©ration Mistral timeout apr√®s {MISTRAL_TIMEOUT}s")
        
        if result['error']:
            raise result['error']
        
        return result['value']
    
    def _robust_json_parse(self, response: str, expected_keys: List[str] = None) -> Dict[str, Any]:
        """Parse JSON robuste avec gestion d'erreurs am√©lior√©e"""
        self.logger.debug(f"Parsing JSON: {response[:200]}...")
        
        # Nettoyer la r√©ponse
        clean_response = response.strip()
        
        # M√©thode 1: Chercher le JSON dans la r√©ponse
        json_patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # JSON simple
            r'\{.*?\}',  # JSON basique
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, clean_response, re.DOTALL)
            for match in matches:
                try:
                    parsed = json.loads(match)
                    if isinstance(parsed, dict):
                        # V√©rifier les cl√©s attendues si sp√©cifi√©es
                        if expected_keys:
                            if any(key in parsed for key in expected_keys):
                                return parsed
                        else:
                            return parsed
                except json.JSONDecodeError:
                    continue
        
        # M√©thode 2: Extraction par regex si JSON introuvable
        self.logger.warning(f"JSON non trouv√©, tentative extraction regex: {clean_response[:100]}")
        
        # Fallback pour classification
        if 'type' in clean_response.lower():
            type_match = re.search(r'["\']?type["\']?\s*:\s*["\']?([^,}"\'\n]+)', clean_response, re.IGNORECASE)
            conf_match = re.search(r'["\']?confidence["\']?\s*:\s*([0-9.]+)', clean_response)
            
            if type_match:
                return {
                    'type': type_match.group(1).strip().strip('"\''),
                    'confidence': float(conf_match.group(1)) if conf_match else 0.5,
                    'reasoning': 'Extraction par regex'
                }
        
        # Fallback final
        self.logger.error(f"Impossible de parser JSON: {clean_response}")
        return {'error': 'Parsing JSON √©chou√©', 'raw_response': clean_response[:200]}
    
    async def _classify_document(self, text: str) -> Dict[str, Any]:
        """Classifie le type de document avec gestion d'erreurs robuste"""
        for attempt in range(MISTRAL_MAX_RETRIES + 1):
            try:
                self.logger.info(f"Classification tentative {attempt + 1}/{MISTRAL_MAX_RETRIES + 1}")
                
                prompt = self.prompts["classification"].format(text=text[:2000])
                self.logger.debug(f"Prompt classification: {prompt[:100]}...")
                
                # G√©n√©ration avec timeout
                response = self._timeout_wrapper(
                    generate,
                    self.model,
                    self.tokenizer,
                    prompt=prompt,
                    max_tokens=100,
                    verbose=False
                )
                
                self.logger.info(f"R√©ponse Mistral classification: {response[:100]}...")
            
                # Parser avec m√©thode robuste
                classification_data = self._robust_json_parse(response, ['type', 'confidence'])
                
                if 'error' in classification_data:
                    if attempt < MISTRAL_MAX_RETRIES:
                        self.logger.warning(f"Tentative {attempt + 1} √©chou√©e, retry...")
                        await asyncio.sleep(1)  # Petite pause avant retry
                        continue
                    else:
                        self.logger.error(f"Toutes les tentatives √©chou√©es: {classification_data}")
                        return {"type": DocumentType.AUTRE, "confidence": 0.2, "reasoning": "Parsing JSON √©chou√©"}
                
                # Mapper vers notre enum
                doc_type_str = classification_data.get("type", "autre").lower()
                try:
                    # Mapper les variantes possibles
                    type_mapping = {
                        'rib': DocumentType.RIB,
                        'relev√© identit√© bancaire': DocumentType.RIB,
                        'facture': DocumentType.FACTURE,
                        'invoice': DocumentType.FACTURE,
                        'contrat': DocumentType.CONTRAT,
                        'contract': DocumentType.CONTRAT,
                        'attestation': DocumentType.ATTESTATION,
                        'courrier': DocumentType.COURRIER,
                        'rapport': DocumentType.RAPPORT,
                        'autre': DocumentType.AUTRE
                    }
                    
                    doc_type = type_mapping.get(doc_type_str, DocumentType.AUTRE)
                except Exception:
                    doc_type = DocumentType.AUTRE
                
                confidence = float(classification_data.get("confidence", 0.5))
                confidence = max(0.0, min(1.0, confidence))  # Clamp entre 0 et 1
                
                result = {
                    "type": doc_type,
                    "confidence": confidence,
                    "reasoning": classification_data.get("reasoning", "")
                }
                
                self.logger.info(f"Classification r√©ussie: {doc_type.value} (conf: {confidence:.2f})")
                return result
                
            except TimeoutError as e:
                self.logger.error(f"Timeout classification tentative {attempt + 1}: {e}")
                if attempt < MISTRAL_MAX_RETRIES:
                    continue
                else:
                    return {"type": DocumentType.AUTRE, "confidence": 0.1, "reasoning": "Timeout Mistral"}
                    
            except Exception as e:
                self.logger.error(f"Erreur classification tentative {attempt + 1}: {e}")
                if attempt < MISTRAL_MAX_RETRIES:
                    await asyncio.sleep(2)  # Pause plus longue en cas d'erreur
                    continue
                else:
                    return {"type": DocumentType.AUTRE, "confidence": 0.0, "reasoning": f"Erreur: {str(e)}"}
        
        # Fallback final si toutes les tentatives √©chouent
        return {"type": DocumentType.AUTRE, "confidence": 0.0, "reasoning": "Toutes tentatives √©chou√©es"}
    
    async def _extract_key_information(self, text: str) -> Dict[str, Any]:
        """Extrait les informations cl√©s du document avec parsing robuste"""
        for attempt in range(MISTRAL_MAX_RETRIES + 1):
            try:
                self.logger.info(f"Extraction informations tentative {attempt + 1}/{MISTRAL_MAX_RETRIES + 1}")
                
                prompt = self.prompts["key_extraction"].format(text=text[:2000])
                self.logger.debug(f"Prompt extraction: {prompt[:100]}...")
                
                # G√©n√©ration avec timeout
                response = self._timeout_wrapper(
                    generate,
                    self.model,
                    self.tokenizer,
                    prompt=prompt,
                    max_tokens=200,
                    verbose=False
                )
                
                self.logger.info(f"R√©ponse Mistral extraction: {response[:150]}...")
                
                # Parser avec m√©thode robuste (sans cl√© 'informations_cles' maintenant)
                key_info = self._robust_json_parse(response, ['dates', 'montants', 'personnes'])
                
                if 'error' in key_info:
                    if attempt < MISTRAL_MAX_RETRIES:
                        self.logger.warning(f"Extraction tentative {attempt + 1} √©chou√©e, retry...")
                        await asyncio.sleep(1)
                        continue
                    else:
                        # Fallback: Extraction basique par regex
                        self.logger.warning("Fallback extraction regex")
                        return self._fallback_extraction(text)
                
                # Valider et nettoyer la structure
                validated_info = self._validate_extracted_info(key_info)
                
                self.logger.info(f"Extraction r√©ussie: {len(validated_info)} cat√©gories")
                return validated_info
                
            except TimeoutError as e:
                self.logger.error(f"Timeout extraction tentative {attempt + 1}: {e}")
                if attempt < MISTRAL_MAX_RETRIES:
                    continue
                else:
                    return self._fallback_extraction(text)
                    
            except Exception as e:
                self.logger.error(f"Erreur extraction tentative {attempt + 1}: {e}")
                if attempt < MISTRAL_MAX_RETRIES:
                    await asyncio.sleep(2)
                    continue
                else:
                    return self._fallback_extraction(text)
        
        # Fallback final
        return self._fallback_extraction(text)
    
    def _validate_extracted_info(self, info: Dict[str, Any]) -> Dict[str, Any]:
        """Valide et nettoie les informations extraites"""
        validated = {
            "dates": [],
            "montants": [],
            "personnes": [],
            "entreprises": [],
            "references": [],
            "autres": {}
        }
        
        for key in validated.keys():
            if key in info:
                value = info[key]
                if isinstance(value, list):
                    validated[key] = [str(item) for item in value if item]  # Nettoyer les valeurs vides
                elif isinstance(value, dict):
                    validated[key] = {k: str(v) for k, v in value.items() if v}
                else:
                    if key == "autres":
                        validated[key] = {"info": str(value)} if value else {}
                    else:
                        validated[key] = [str(value)] if value else []
        
        return validated
    
    def _fallback_extraction(self, text: str) -> Dict[str, Any]:
        """Extraction de secours par regex si Mistral √©choue"""
        self.logger.info("Utilisation extraction fallback par regex")
        
        result = {
            "dates": [],
            "montants": [],
            "personnes": [],
            "entreprises": [],
            "references": [],
            "autres": {"extraction_type": "fallback_regex"}
        }
        
        # Extraction dates
        date_patterns = [
            r'\b\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4}\b',
            r'\b\d{1,2}\s+[a-z√©√®√™√¥√†]+\s+\d{4}\b'
        ]
        for pattern in date_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            result["dates"].extend(matches[:5])  # Limiter √† 5
        
        # Extraction montants
        montant_patterns = [
            r'\b\d+[,\.]\d{2}\s*‚Ç¨\b',
            r'\b\d+[,\.]\d{2}\s*euros?\b',
            r'\btotal\s*:?\s*\d+[,\.]\d{2}\b'
        ]
        for pattern in montant_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            result["montants"].extend(matches[:3])  # Limiter √† 3
        
        # Extraction r√©f√©rences
        ref_patterns = [
            r'\bn[u¬∞\s]*\d{6,}\b',
            r'\bref[\s\.:]*\w+\d+\b',
            r'\bfacture[\s\.:]*\w*\d+\b'
        ]
        for pattern in ref_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            result["references"].extend(matches[:3])
        
        # Nettoyer et d√©dupliquer
        for key in ["dates", "montants", "references"]:
            result[key] = list(set(result[key]))[:5]  # D√©doublonner et limiter
        
        return result
    
    async def _summarize_document(self, text: str) -> str:
        """Cr√©e un r√©sum√© du document"""
        try:
            # Nettoyer le texte d'entr√©e
            clean_text = text.replace('\n', ' ').strip()[:1500]
            prompt = self.prompts["summarization"].format(text=clean_text)
            
            self.logger.info(f"G√©n√©ration r√©sum√© avec prompt de {len(prompt)} caract√®res")
            
            response = generate(
                self.model,
                self.tokenizer,
                prompt=prompt,
                max_tokens=200,
                verbose=False
            )
            
            self.logger.info(f"R√©ponse Mistral brute: {response[:100]}...")
            
            # Nettoyer la r√©ponse en profondeur
            summary = response.strip()
            
            # Supprimer le prompt entier qui peut √™tre r√©p√©t√©
            prompt_phrases = [
                "Tu es un expert en synth√®se documentaire",
                "Cr√©√© un r√©sum√© concis",
                "Document :",
                "R√©sum√© en 2-3 phrases",
                "maximum, en fran√ßais professionnel",
                "sans reprendre le prompt"
            ]
            
            for phrase in prompt_phrases:
                if phrase in summary:
                    parts = summary.split(phrase)
                    summary = parts[-1].strip()
            
            # Supprimer les √©ventuels pr√©fixes et suffixes
            prefixes_to_remove = [":", "Document", "R√©sum√©", "-"]
            for prefix in prefixes_to_remove:
                if summary.startswith(prefix):
                    summary = summary[len(prefix):].strip()
            
            # Supprimer les r√©p√©titions de texte original
            if len(summary) > 200 and clean_text[:50] in summary:
                # Le r√©sum√© contient le texte original, on extrait seulement la fin
                summary_start = summary.rfind(clean_text[:50])
                if summary_start > 0:
                    summary = summary[summary_start + len(clean_text[:50]):].strip()
            
            # Nettoyer les lignes vides et les caract√®res ind√©sirables
            summary = ' '.join(summary.split())
            
            # Si le r√©sum√© contient encore des fragments du prompt, le raccourcir
            if any(p in summary.lower() for p in ["expert", "analyse", "document :", "r√©sum√©"]):
                sentences = summary.split('.')
                clean_sentences = []
                for sentence in sentences:
                    if not any(p in sentence.lower() for p in ["expert", "analyse document", "tu es"]):
                        clean_sentences.append(sentence.strip())
                summary = '. '.join(clean_sentences).strip()
                if summary and not summary.endswith('.'):
                    summary += '.'
            
            # Validation finale
            if len(summary) < 10:
                summary = f"Document analys√© contenant {len(text.split())} mots. Contenu principal extrait."
            
            return summary[:500]  # Limiter la taille
            
        except Exception as e:
            self.logger.error(f"Erreur r√©sum√© : {e}")
            return f"Document trait√© automatiquement. Contenu de {len(text.split()) if text else 0} mots analys√©."
    
    async def _analyze_compliance(self, text: str) -> Dict[str, Any]:
        """Analyse la conformit√© du document"""
        try:
            prompt = self.prompts["compliance"].format(text=text[:1500])
            
            response = generate(
                self.model,
                self.tokenizer,
                prompt=prompt,
                max_tokens=200,
                verbose=False
            )
            
            # Parser la r√©ponse JSON
            try:
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                json_str = response[json_start:json_end]
                
                compliance_data = json.loads(json_str)
                return compliance_data
                
            except (json.JSONDecodeError, ValueError) as e:
                self.logger.warning(f"Erreur parsing compliance JSON : {e}")
                return {"conformite": "non_√©valu√©", "error": "Erreur de parsing"}
                
        except Exception as e:
            self.logger.error(f"Erreur analyse conformit√© : {e}")
            return {"conformite": "erreur", "error": str(e)}
    
    def get_supported_document_types(self) -> List[str]:
        """Retourne la liste des types de documents support√©s"""
        return [doc_type.value for doc_type in DocumentType]
    
    def get_supported_analysis_types(self) -> List[str]:
        """Retourne la liste des types d'analyse support√©s"""
        return [analysis_type.value for analysis_type in AnalysisType]


# Service FastAPI
document_analyzer = DocumentAnalyzer()

app = FastAPI(
    title="Document Analyzer Service (MLX Native)",
    description="Service d'analyse documentaire intelligent utilisant Mistral 7B MLX",
    version="1.0.0"
)

# Stockage des conversations en m√©moire (simple pour MVP)
conversations_storage = {}

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:8000",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:8000",
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "DELETE"],
    allow_headers=["*"],
)


@app.on_event("startup")
async def startup_event():
    """Initialise le service au d√©marrage"""
    await document_analyzer.initialize()


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "document_analyzer",
        "model_loaded": document_analyzer.model is not None,
        "supported_types": document_analyzer.get_supported_document_types(),
        "supported_analyses": document_analyzer.get_supported_analysis_types()
    }


@app.post("/analyze")
async def analyze_document_endpoint(request: DocumentAnalysisRequest):
    """Analyse un document"""
    try:
        # Convertir les strings en enums
        analysis_types = []
        for analysis_str in request.analysis_types:
            try:
                analysis_types.append(AnalysisType(analysis_str))
            except ValueError:
                raise HTTPException(status_code=400, detail=f"Type d'analyse non support√©: {analysis_str}")
        
        result = await document_analyzer.analyze_document(
            text=request.text,
            analysis_types=analysis_types,
            document_context=request.document_context
        )
        
        # Convertir le r√©sultat pour la s√©rialisation JSON
        return {
            "success": True,
            "result": {
                "document_type": result.document_type.value,
                "confidence": result.confidence,
                "summary": result.summary,
                "key_information": result.key_information,
                "entities": result.entities,
                "sentiment": result.sentiment,
                "compliance_status": result.compliance_status,
                "processing_time": result.processing_time
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur endpoint analyze : {e}")
        return {"success": False, "error": str(e)}


@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """Endpoint de chat conversationnel avec Mistral"""
    try:
        # Prompt syst√®me par d√©faut
        default_system_prompt = """Tu es Mistral, un assistant IA sp√©cialis√© dans l'analyse de documents administratifs pour LEXO v1.

CONTEXTE:
- Tu aides les utilisateurs √† analyser leurs documents (factures, RIB, imp√¥ts, attestations, contrats)
- Tu peux expliquer le contenu des documents, extraire des informations cl√©s, et r√©pondre aux questions
- Les documents ont d√©j√† √©t√© trait√©s par OCR et analyse pr√©liminaire

CAPACIT√âS:
- Analyse et r√©sum√© de documents
- Extraction d'informations cl√©s (dates, montants, entit√©s)
- Classification et cat√©gorisation
- R√©ponse aux questions sur le contenu
- Conseils sur l'organisation documentaire

STYLE:
- Sois professionnel mais accessible
- Utilise un langage clair et pr√©cis
- Structure tes r√©ponses avec des puces ou sections si n√©cessaire
- Cite les √©l√©ments sp√©cifiques du document quand pertinent
- Si tu n'es pas s√ªr, indique-le clairement

S√âCURIT√â:
- Ne jamais r√©v√©ler d'informations personnelles sensibles
- Respecter la confidentialit√©
- Signaler si un document semble contenir des donn√©es sensibles"""

        # Utiliser le prompt syst√®me fourni ou celui par d√©faut
        system_prompt = request.system_prompt or default_system_prompt
        
        # Construire le prompt complet
        full_prompt = f"{system_prompt}\n\nUser: {request.message}\nAssistant:"
        
        # G√©n√©rer la r√©ponse avec Mistral
        if document_analyzer.model is None:
            raise HTTPException(status_code=503, detail="Mod√®le Mistral non charg√©")
        
        start_time = time.time()
        
        # G√©n√©rer la r√©ponse
        response = generate(
            document_analyzer.model,
            document_analyzer.tokenizer,
            prompt=full_prompt,
            max_tokens=request.max_tokens,
            verbose=False
        )
        
        processing_time = time.time() - start_time
        
        # Nettoyer la r√©ponse (retirer le prompt)
        if response.startswith(full_prompt):
            response = response[len(full_prompt):].strip()
        
        # Compter les tokens (approximation)
        tokens_used = len(response.split())
        
        # Stocker dans l'historique des conversations
        if request.conversation_id not in conversations_storage:
            conversations_storage[request.conversation_id] = {"messages": []}
        
        # Ajouter le message utilisateur et la r√©ponse √† l'historique
        conversations_storage[request.conversation_id]["messages"].extend([
            {
                "role": "user",
                "content": request.message,
                "timestamp": time.time()
            },
            {
                "role": "assistant", 
                "content": response,
                "timestamp": time.time(),
                "tokens_used": tokens_used
            }
        ])
        
        return {
            "response": response,
            "conversation_id": request.conversation_id,
            "tokens_used": tokens_used,
            "processing_time": processing_time,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Erreur endpoint chat : {e}")
        raise HTTPException(status_code=500, detail=f"Erreur de g√©n√©ration: {str(e)}")


@app.get("/conversations/{conversation_id}")
async def get_conversation_history(conversation_id: str):
    """R√©cup√©rer l'historique d'une conversation"""
    try:
        conversation = conversations_storage.get(conversation_id, {"messages": []})
        return {
            "conversation_id": conversation_id,
            "messages": conversation["messages"]
        }
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration conversation : {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/conversations/{conversation_id}")
async def clear_conversation(conversation_id: str):
    """Effacer une conversation"""
    try:
        if conversation_id in conversations_storage:
            del conversations_storage[conversation_id]
        return {
            "success": True,
            "message": f"Conversation {conversation_id} effac√©e"
        }
    except Exception as e:
        logger.error(f"Erreur effacement conversation : {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/document-types")
async def get_document_types():
    """Retourne les types de documents support√©s"""
    return {
        "document_types": document_analyzer.get_supported_document_types()
    }


@app.get("/analysis-types")  
async def get_analysis_types():
    """Retourne les types d'analyse support√©s"""
    return {
        "analysis_types": document_analyzer.get_supported_analysis_types()
    }


if __name__ == "__main__":
    print("üîç D√©marrage Document Analyzer Service MLX")
    print("Port: 8004")
    print("Mod√®le: Mistral-7B-Instruct-v0.3-4bit")
    
    uvicorn.run(
        "document_analyzer:app",
        host="127.0.0.1",
        port=8004,
        reload=False,
        log_level="info"
    )